
# W&B specifications
enable_wandb_logging: !!bool True
wandb_project_name: "gpt2_small"
experiment_name: 'gpt2'
logs_dir: 'logs' # W@B files are stored here.


# Run specifications
init_model_from: '' # 'checkpoint' 'scratch', 'checkpoint', 'pre_trained'


max_iters: !!int 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens
evaluation_interval: !!int 500 # set to 0 or None to disable evaluation
checkpoint_interval: !!int 5000 # Note that: checkpoint_interval % evaluation_interval = 0

checkpoint_dir: 'checkpoints'
checkpoint_file: 'model_00016_2024-11-14.pt' # Reads the checkpoint_dir

# Embedding layer specification
sequence_length: 1024 # Maximum sequence/context length (tokens) or block size (variable T in the code)
vocab_size: 50257 # Vocabulary size: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token 
embedding_dim: 768 # Embedding dimension  (variable D in the code)

# Transformer blocks specification
n_layer: 12 # number of layers
n_head: 12 # number of heads

# Batch specification for Gradient accumulation
total_batch_size: 524288 # Total tokens per batch (2**19 ~ 0.5M tokens)
micro_batch_size: 16 # micro batch size (variable B in the code)
# sequence length is specified above (= 1024)

# Training Hyperparameters
max_lr: !!float 6e-4
min_lr: !!float 6e-5 # max_lr * 0.1
warmup_iters: !!int 715


device_type: "cuda" # or "cpu" detected automatically unless forced here